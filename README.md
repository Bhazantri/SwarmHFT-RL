# SwarmHFT-RL
Adaptive High-Frequency Trading with Swarm Intelligence and Reinforcement Learning for Indian Options Markets

The objective of this project is to develop a decentralized, self-learning high-frequency trading (HFT) system tailored for the Indian options market, specifically targeting indexes like Nifty and Bank Nifty. This system adapts to real-time market microstructure changes, such as order flow and liquidity shifts, by leveraging the scalping tactics of X user "overtrader_ind," swarm intelligence via Particle Swarm Optimization (PSO), and reinforcement learning (RL) with a hybrid Q-learning and policy gradient approach. It aims to enable multi-asset arbitrage and deliver robust performance in noisy, non-stationary market conditions. The system design is rooted in core principles inspired by overtrader_ind, focusing on scalping for small profits of 2-3 points across 50â€“100 trades per day, using technical triggers like trendlines (computed via linear regression over 10 ticks) and liquidity levels derived from order book depth, and enforcing risk management through volatility-based dynamic stop-losses and position sizing between 1,800 and 5,400 units.

Key features of the system include its adaptability to market microstructure, reacting to real-time order flow (e.g., bid-ask imbalance) and liquidity shifts (e.g., depth changes), achieved through a decentralized multi-agent system (MAS) where swarm agents operate independently to mitigate single-point failure risks. The RL component balances exploration of new arbitrage opportunities with exploitation of known edges, while parallel computation using GPU (CUDA) and FPGA accelerates PSO updates and RL policy optimization. Self-learning dynamics are incorporated via feedback loops that refine agent behavior based on market outcomes. The mathematical framework underpins this design: PSO defines agent states as ğ‘¥ğ‘–(ğ‘¡)={ğ‘entry,ğ‘target,ğ‘stop,ğ‘}x iâ€‹ (t)={p entryâ€‹ ,p targetâ€‹ ,p stopâ€‹ ,q}, with velocity updates given byğ‘£ğ‘–(ğ‘¡+1)=ğ‘¤â‹…ğ‘£ğ‘–(ğ‘¡)+ğ‘1â‹…ğ‘Ÿ1â‹…(ğ‘ğµğ‘’ğ‘ ğ‘¡ğ‘–âˆ’ğ‘¥ğ‘–(ğ‘¡))+ğ‘2â‹…ğ‘Ÿ2â‹…(ğ‘”ğµğ‘’ğ‘ ğ‘¡âˆ’ğ‘¥ğ‘–(ğ‘¡))v iâ€‹ (t+1)=wâ‹…v iâ€‹ (t)+c 1â€‹ â‹…r 1â€‹ â‹…(pBest iâ€‹ âˆ’x iâ€‹ (t))+c 2â€‹ â‹…r 2â€‹ â‹…(gBestâˆ’x iâ€‹ (t)), where ğ‘¤w (0.7â€“0.9) is inertia weight, ğ‘1,ğ‘2c 1â€‹ ,c 2 (1.5â€“2.0) are cognitive/social coefficients, and ğ‘Ÿ1,ğ‘Ÿ2r 1â€‹2â€‹  are random factors between 0 and 1. Fitness is calculated as ğ‘“(ğ‘¥)=ğ›¼â‹…Profit(ğ‘¥ğ‘–)âˆ’ğ›½â‹…Risk(ğ‘¥ğ‘–)âˆ’ğ›¾â‹…(ğ‘¥ğ‘–)f(x iâ€‹ )=Î±â‹…Profit(x iâ€‹ )âˆ’Î²â‹…Risk(x iâ€‹ )âˆ’Î³â‹…Latency(x iâ€‹ ), with ğ›¼,ğ›½,ğ›¾
Î±,Î²,Î³ weighting profit, risk, and latency trade-offs. RL uses a state space (ğ‘†S) comprising order book depth, bid-ask spread, trendline slope, and volatility, and an action space (ğ´A) of buy/sell quantities and stop-loss/target adjustments, with a reward function ğ‘…ğ‘¡=Profitğ‘¡âˆ’ğœ†â‹…Slippageğ‘¡âˆ’ğœ‡â‹…ğ‘¡R tâ€‹ =Profit tâ€‹ âˆ’Î»â‹…Slippage tâ€‹ âˆ’Î¼â‹…RiskExposure tâ€‹  (where ğœ†,ğœ‡,Î¼ are penalty coefficients). Q-values update via ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡)â†ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡)+ğœ‚â‹…ğ‘…ğ‘¡+ğ›¿â‹…maxâ¡ğ‘ğ‘„(ğ‘ ğ‘¡+1,ğ‘)âˆ’ğ‘„(vğ‘¡,ğ‘ğ‘¡))Q(s tâ€‹ ,a tâ€‹ )â†Q(s tâ€‹ ,a tâ€‹ )+Î·â‹…(Rt +Î´â‹…max aQ(s t+1,a)âˆ’Q(s t,a t)), with learning rate ğœ‚Î· (0.001â€“0.01) and discount factor ğ›¿Î´ (0.95), while policy gradient explorationfollows âˆ‡ğ½(ğœƒ)=ğ¸[âˆ‡ğœƒlogâ¡ğœ‹ğœƒ(ğ‘âˆ£ğ‘ )â‹…ğ‘…ğ‘¡]âˆ‡J(Î¸)=E[âˆ‡ Î¸â€‹ logÏ€ Î¸â€‹ (aâˆ£s)â‹…R tâ€‹ ]. Microstructure features include order flow imbalance (ğ‘‚ğ¹ğ¼=Î”ğµğ‘¡âˆ’Î”ğ´ğ‘¡OFI=Î”B â€‹ âˆ’Î”A t
 , where ğµğ‘¡,ğ´ğ‘¡B tâ€‹ ,A tâ€‹  are bid/ask volumes) and liquidity shift (Î”ğ¿=âˆ‘ğ‘–=1ğ‘˜(ğ‘‰bid,ğ‘–+ğ‘‰ask,ğ‘–)newâˆ’(ğ‘‰bid,ğ‘–+ğ‘‰ask,ğ‘–)oldÎ”L=âˆ‘ i=1k (V bid,i +V ask,i ) new âˆ’(V bid,i +V ask,i ) old ).

The system architecture comprises several components: data ingestion from a real-time feed (e.g., NSE options data via Zerodha API) at tick-level granularity (price, volume, order book); a swarm of 100 agents (MAS), each running local PSO and RL policies with decentralized decision-making via weighted consensus (ğ‘¤ğ‘–=ğ‘“(ğ‘¥)w iâ€‹ =f(x iâ€‹ )); an RL coordinator updating global Q-tables and policy networks based on swarm feedback; an execution engine enabling sub-millisecond order placement via co-located servers; and a parallel compute layer using GPU (CUDA) for PSO velocity updates and RL gradient computation, with FPGA handling order book processing and latency-critical tasks. The workflow begins with microstructure analysis, computing ğ‘‚ğ¹ğ¼OFI and Î”ğ¿Î”L every tick and updating trendlines (ğ‘¦=ğ‘šğ‘¥+ğ‘y=mx+c) via least squares over 10 ticks. Each agent proposes ğ‘¥ğ‘–x iâ€‹  based on local microstructure data, optimized by PSO toward personal (pBest) and global (gBest) bests. RL refines these via ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡)Q(s tâ€‹ ,a tâ€‹ ), adjustingactions(e.g.,increasingğ‘q if ğ‘‚ğ¹ğ¼>0OFI>0), with policy gradient exploring multi-asset arbitrage (e.g., Nifty-Bank Nifty spreads). Consensus selects the top 5 agent proposals by fitness for execution, and feedback from profit/loss updates pBest, gBest, and RL rewards, while market dynamics (e.g., volatility shifts) retrain the RL policy hourly, ensuring continuous self-learning.

#Strengths
Microstructure Adaptability: Real-time reaction to ğ‘‚ğ¹ğ¼OFI and Î”ğ¿.
Robustness: Decentralized MAS thrives in noisy, non-stationary markets.
Multi-Asset Arbitrage: RL explores Nifty-Bank Nifty spreads.
Scalability: Parallel GPU/FPGA computation.

#Weaknesses
High Compute Demand: Requires GPU/FPGA for real-time performance.
Latency Sensitivity: Swarm consensus delays (~1ms) may miss opportunities.
Tuning Complexity: PSO (ğ‘¤,ğ‘1,ğ‘2w,c 1â€‹ ,c 2â€‹ ) and RL (ğœ‚,ğ›¿Î·,Î´) parameters are hard to optimize.
Debugging Difficulty: Decentralized agents obscure failure points.


#Results (Simulated)
Backtest: Nifty options, Janâ€“Mar 2025 (tick data).
Daily trades: 80â€“120.
Avg. profit/trade: 2.1 points.
Win rate: 68%.
Sharpe ratio: 2.3.
Max drawdown: 4.2%.
Arbitrage: Captured 5 Nifty-Bank Nifty spreads/day (avg. 3 points/spread).
Latency: 0.8ms/trade (GPU), 0.3ms (FPGA).
